1. Why do we need machine learning methods instead of creating a complicated formula?
Ans:
Machine learning can deal with big data problems,find a certain feature for a large amount of data,or statistical probability,and model,use the model
to solve the problem,but creating a complicated formula can not completely fit the rules in a large number of data,And the model fitted by machine learning
is more suitable for similar data.
2. What's the disadvantages of the 1st Random Choosen methods in our course?
Ans:
Random selection of a directin can not guarantee the correct direction,so it is easy to do useless work,can not just reduce the loss just right and optimize the model
3. Is the 2nd method supervised direction better than 1st one? What's the disadvantages of the 2nd supversied directin method?
Ans:
Yes,the 2nd method is better than 1st one,the disadvantages of the 2nd supversied direction method is that although there is a directional adjustment, it can only continue in the right direction. If the direction is wrong, it cannot be adjusted to the correct direction.
4. Why do we use Derivative / Gredient to fit a target function?
Ans:
because using Derivative can find the best direction to reduce the loss function.
5. In the words 'Gredient Descent', what's the Gredient and what's the Descent?
Ans:
The original meaning of the gradient is a vector (vector), which means that the direction derivative of a function at that point takes the maximum value along the direction, that is, the function changes the fastest along the direction (the direction of the gradient) at that point, and the change Maximum rate (module for this gradient)
The meaning of Descent is that decreasing the loss function to the minimum
6. What's the advantages of the 3rd gradient descent method compared to the previous methods?
Ans:
Gradient descent is a first-order iterative optimization algorithm for finding the minimum of a function. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient (or approximate gradient) of the function at the current point. If, instead, one takes steps proportional to the positive of the gradient, one approaches a local maximum of that function; the procedure is then known as gradient ascent.It can guarantee the correct direction,so it can reduce the loss and optimize the model maximized
7. Using the simple words to describe: What's the machine leanring.
Ans:
Machine learning is a subfield of computer science that evolved from the study of pattern recognition and computational learning theory in artificial intelligence